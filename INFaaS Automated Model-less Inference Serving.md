## INFaaS Automated Model-less Inference 

#### Abstract

虽然目前已经有不少的工作研究ML推理服务，但是推理服务系统的易用性和经济效益仍然是一大挑战。对于常用的推理服务系统，用户需要自行在包含数千个模型的变体空间中选择合适的模型变体来满足自身多样的应用需求，这些模型变体往往来自以训练好的模型，在适用的硬件、资源使用、延迟、开销和精确度等方面具有不同的表现。然而，由于需求、查询负载和应用都在不断地演变，需要为每个推理查询动态地选择模型变体等，从而避免简单的自扩展造成过量的开销。在部分系统中，为了简化决策，服务提供者会固定一个特定的模型变体供用户使用，并且在负载增加时直接将其复制，从而避免了在广大的变体空间中搜索的问题。然而，这种方法忽略了云中模型变体和硬件平台的多样性，缺乏对决策空间的理解，往往给用户造成了不小的开销

本文介绍了一个用于分布式推理服务的自动化无模型系统INFaaS。用户只需要简单地在INFaaS上声明应用对性能和准确度的需求，由系统代表用户在广大的决策空间中进行搜索，自动地为查询选择合适的模型、硬件架构和模型优化，满足用户的应用需求。此外，INFaaS还能自动利用已训练好的模型生成模型变体，且将VM层级的水平扩展和模型层级的水平扩展结合，使得不同的模型变体能够运行在合适的机器上为用户提供良好的服务。与AWS EC2上最先进的推理服务系统相比，INFaaS有1.3×的吞吐量，更少违背延迟目标，且最多能够节约21.6×的开销(平均为8.5×)，在性能和开销上均有良好的表现



#### Introduction

目前而言，依赖ML模型推理服务的应用的数量已经非常庞大，且这个数量预计还会不断增长。分布式推理在ML产业的开销中占据了主导地位，例如在AWS中，推理服务开销占据了ML基础设施开销的90%

一个ML的计算周期通常分为两个部分：模型训练和模型推理。在模型训练阶段，模型通过输入测试用例进行训练，通常需要长时间的超参数搜索、专用的硬件资源以及没有运行时间限制(模型会被不断训练使其更加准确)。在推理阶段，被训练好的模型会接收来自不同应用的查询，并返回推理结果。由于推理服务是面向用户的，在延迟方面具有较高要求，因此推理服务系统在处理难以预测的爆发性的请求时，应当依据延迟限制进行负载预测，进行相应的扩展，从而避免无法及时响应的问题

推理服务系统的决策往往涉及到了以下因素

- Diverse application requirements

  应用会发出具有不同需求的请求，这些需求包括延迟、开销、精确度、隐私等。下图显示了同样的人脸识别模型会被不同的应用以不同的需求进行查询。

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220904111959294.png" alt="image-20220904111959294" style="zoom:50%;" />

  对于形如Intruder Detection这种应用而言，它更注重于响应时间上的要求，而可以容忍部分的不准确。而相比之下，形如在社交媒体上标记人类这类应用，对精确度的要求很高，而可以容忍较长一些的延迟

- Heterogeneous execution environments

  使用各种异构的硬件资源(如不同版本的CPU、GPU和TPU等)有助于我们满足应用多样的需求和负载的动态变化。然而，对这些异构硬件资源的管理和扩展增加了管理的复杂性

- Diverse model-variants

  在模型训练中采用不同的训练框架、不同的训练方法等，会为一个模型产生多样的模型变体。这些模型变体在适用硬件、开销、准确度等方面具有不同的表现

如果把这些因素结合起来考虑，将会形成一个巨大的决策空间。例如，对于21个已训练好的图像分类模型，我们可以通过应用模型图优化器、为不同batch大小进行优化、针对适用底层硬件进行优化等方法，构建出166个不同的模型变体。这些模型变体在精确度、模型加载时延、模型推理时延、底层硬件开销等方面均有不同的表现。每当新的硬件加速器或优化方案出现，都会形成更多的模型变体

这样巨大的决策空间使得用户无法手动地将每个推理查询的需求映射到模型变体、底层硬件设施和自动扩展参数的选择上。如果需要考虑负载变化、应用演变和硬件资源的可用性等问题，选择的复杂性将会更高。因此，由用户自行进行这些决策往往是不可行的。此外，由于模型推理服务的每个查询请求涉及的数据大小、性能需求等不同，采用静态决策的方式选择固定的模型变体等也是不合适的

虽然大量模型变体的出现，给我们带来了搜索上的挑战，但这同样也是重要的机遇：选择合适的模型变体能够让我们满足多样的在性能、开销和精确度等方面的需求，并能够适应变化的负载和硬件可用性。然而，由于决策空间带来的复杂性，已有的系统往往忽略了模型变体选择带来的机遇，而是要求用户自行根据需求选择对应的模型变体和底层硬件等，但这在开销等方面带来了不可忽视的影响。因此，作者认为分布式推理服务系统应当代表用户自动化地做出决策，选择合适的模型变体来执行推理工作，并且能够自行管理模型变体和异构资源

基于这些原因，作者提出了INFaaS分布式推理服务系统，采用自动化和model-less的模式为用户提供推理服务。INFaaS通过采用一个model-less的接口，使用户只需要为他们的推理查询声明性能、开销或精确度的需求。INFaaS能够自动生成载入的模型的模型变体，在大的决策空间中选择合适的模型变体，并自动在不同优化的变体之间切换，以最好地满足查询要求。另外，INFaaS还提供了为模型变体自动分配资源和在异构集群中调度查询的功能

**Introduciton接下来的部分讲述了一些具体实现的方法，但是光看这些方法难以理解其实现的方式和效果，需要结合着后面的具体方法进行理解。INFaaS的核心功能就是为不同需求的请求选择最合适的模型变体，并且能够进行自动化的扩展**



#### Challenges

本节中提出了目前的推理系统面临的困难，即INFaaS应当解决的问题

- Selecting the right model-variant(为请求选择合适的变体)

  问题背景：一个模型的模型变体主要由几个方面进行区分，包括采用的硬件架构(如ResNet50)、采用的编程框架(如PyTorch)、模型的图优化器(如TensorRT)、模型的超参数(如适用的batch大小)以及适用的硬件平台(如CPU、TPU、GPU等)。基于21个图像分类模型和AWS EC2上支持的硬件平台，可以划分出接近4032个不同的模型变体。这些模型变体在性能、开销和精确度等方面的表现均有不同，进而给我们带来了巨大的决策空间。随着新的推理加速器和优化技术的出现，模型变体的数量只会不断增加

  目前的做法：目前已有的推理服务系统需要用户自行选择模型变体来满足应用的需求。然而，要正确选择并使用这些模型变体要求用户应对编程框架、模型图优化器和硬件架构等有充分的了解，因此这些系统通常限制用户可以使用的变体，只保留易于理解和使用的部分。

  期望功能：推理服务系统应当能够自动且高效地为用户的查询选择合适的模型变体，从而满足应用的需求

- Reducing cost as load varies(在负载变化时，以最低的开销满足需求)

  问题背景：由于查询模式和应用的SLO指标通常是难以预测的，而根据峰值需求来配置实例会造成巨大的开销，因此推理服务系统应具备根据变化动态调整的能力

  目前推理系统的做法和存在的问题：传统的推理服务系统通过水平扩展VM并在VM上应用重复的模型变体(这些系统采用的模型变体通常是固定的)来为查询请求提供对应的服务。然而，静态地确定模型变体是不准确的，因为负载的变化往往导致了合适的变体的变化(例如一个新的CPU变体更适用于低负载的情况来降低开销)，且复制相同变体所需的硬件资源不一定是可用的

  期望的解决方式：推理服务系统在进行扩展时，应当再次权衡并选择最合适的变体来应对负载的变化。这个方法带来的挑战在于如何选择最合适的变体，使其能够在可用的硬件资源上运行，且能够满足查询需求。以下面的两个表为例，说明在不同负载数量和需求情况下，我们最优的变体组合是不同的。在QPS=10,SLO=300的条件下，请求负载较低且对延迟的要求不高，我们使用两个变体A来满足需求是最合适的。而在QPS=1000,SLO=300的情况下，请求负载高但对延迟要求不高，我们使用最昂贵的变体C比使用大量的A要便宜，因为C具备更好的大规模处理能力

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220904171855812.png" alt="image-20220904171855812" style="zoom:50%;" />

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220904171905607.png" alt="image-20220904171905607" style="zoom:50%;" />

- Improving utilization at low load(提升资源利用率)

  目前的做法带来的问题：为了能够获得可预测的性能，目前一些系统令每个模型变体独立地运行在专用的设备上，从而互斥地访问硬件资源。这种做法往往引起了硬件资源的低利用率，特别是在负载较低的情况

  期望的解决方式：推理服务系统应当能支持多租户的方式，允许不同应用和变体之间能够共享硬件资源，进而提升资源的利用率和降低整体成本。与训练任务相比，ML推理任务对计算和内存资源的需求更低，因此对GPU和加速器的共享更容易实现，能够带来更大的收益

  带来的挑战：由于在负载提高时，使用同样设备的变体往往会因资源竞争导致性能损失(资源共享引起性能损失的点通常因模型而异，且取决于负载类型和硬件架构)，因此在保持可预测的性能的情况下进行资源的共享并不容易。



#### INFaaS系统介绍

- 设计准则与功能

  1. 设计准则

     第一，INFaaS应当提供的是声明式API，使得用户不需要指定具体的模型、模型变体、硬件平台以及自动化扩展参数等。用户只需要说明高层级的开销、性能或准确度的需求

     第二，INFaaS应当能够在考虑到模型变体的变化状态和硬件设备的可用性的同时，自动且高效地选择合适的模型变体，使得系统能够为查询提供服务，并能够应对未来应用负载的变化

     第三，INFaaS应当能够在确保不违背性能-开销约束的情况下，通过硬件资源共享来提升资源利用率

     最后，系统设计应当是模块化的且具备可扩展性，从而能够允许修改模型变体的选择策略

  2. 系统功能

     - 模型变体生成：INFaaS能够根据用户提交的模型生成新的模型变体，并把这些变体保存到仓库中。生成的模型变体是利用模型图优化、修改batch大小等方式在不同维度(性能、精确度、开销)进行优化获得的
     - 模型变体选择：对于每一条查询，INFaaS会自动化选择合适的模型变体来满足查询请求在性能、开销、精确度等方面的需求。INFaaS通过高效地维护模型变体和硬件资源的静态、动态信息，从而支持低时延的模型变体选择和扩展
     - 自动化扩展：INFaaS的自动扩展器将VM层级的扩展与模型层级的水平和垂直扩展结合，从而在提升资源利用率的同时，满足应用在性能和开销方面的需求。INFaaS引入了模型层级的垂直扩展，即在运行过程中可以通过模型选择将正在使用的模型变体变换为另一个优化方向不同的模型变体，使其更适合目前的使用
     - 可变策略：INFaaS具有默认的模型变体选择策略，但也支持用户扩展和自定义选择策略

- Model-less interface for inference(推理服务的接口)

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220905164812976.png" alt="image-20220905164812976" style="zoom:50%;" />

  模型登记接口register_model：用户通过这个API提交一个或者多个模型。API支持用户声明模型的标识符、模型数据以及应用标识符(说明模型用于什么类型的任务)。如果是用于同个应用的不同预测任务的模型，可以通过appID区分它们的用途。INFaaS会根据用户提交的已训练好的模型，对其batch大小、适用硬件和模型图优化器进行修改，生成不同的模型变体(INFaaS不进行模型训练，只对训练好的模型生成模型变体)。另外，该接口通过接收一个标准的数据集valSet，计算出模型变体的准确度

  查询提交接口：INFaaS支持用户采用两种不同的方式提交查询请求。第一，用户可以指定查询请求在性能、开销和准确度方面的需求(latency和accuracy)，并说明使用的应用(appID)，由INFaaS在对应应用的决策空间中选择合适的变体和扩展策略对查询进行响应。第二，用户可以直接指定处理查询请求的模型，由INFaaS选择合适的模型变体并为指定的模型变体进行资源分配

- 查询请求的服务流程

  应用通过前端(通常位于控制器)向INFaaS提交查询请求，由控制器选择一个合适的模型变体并将推理请求分派到一个Worker中。Worker根据选择的模型变体，将推理请求发送给恰当的Hardware Executors执行推理任务，最终将推理结果返回给应用

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220905171630914.png" alt="image-20220905171630914" style="zoom:50%;" />

  

#### Architecture(INFaaS系统架构设计)

INFaaS系统具体包含了以下几个组件(在上述流程中也有呈现)

- Controller

  INFaaS控制器是接收模型登记和推理请求的核心组件，其主要由三个模块构成：Dispatcher、VM-Autoscaler、Model Registrar。Dispatcher在接收到推理请求时，根据模型变体选择策略选择合适的模型变体执行推理任务；VM-Autoscaler根据当前的负载情况以及资源的利用率，增加或减少Worker的数量；Model Registrar负责处理模型登记请求

- Workers

  Workers的主要功能在于执行Controller分派的推理任务。Hardware Executor(每类硬件有特定的守护进程，称为Hardware-specific)负责模型变体的部署和运行；Dispatcher根据选择的模型变体将推理请求转发给对应的Hardware Executor，进行推理任务；Model-Autoscaler通过检测负载的变化，根据模型变体选择策略选择一个模型变体进行扩展或收缩；Monitoring Daemon通过检测硬件的利用率和模型变体的资源使用，调整模型变体间共享的硬件资源，避免违背SLO约束

- Variant-Generator and Variant-Profiler

  根据已登记的模型，Variant-Generator利用模型图优化器(如TensorRT和Neuron)，生成适合不同batch大小、硬件和硬件参数的模型变体。INFaaS利用用户提供的验证集，计算出新生成的变体的准确率，并将其记录在Metadata Store中

  为了帮助模型变体的选择，Variant-Profiler对每个模型变体进行一次分析，并测量一些统计数据，如载入时延、推理时延、峰值内存利用率(应当是在运行验证集的同时进行分析)。测量出的模型变体的统计数据协同相应的appID、精确度和支持的batch大小将被记录在Metadata Store中。在分析完毕后，模型变体将被保存在Model Repository中

- Model-Variant Selection Policy

  模型变体选择策略分为两种情况下的选择

  1. 请求到达

     控制器的 Dispatcher 使用该策略为接收到的每个查询请求确定一个执行的模型变体。选择策略应用在推理服务的关键路径上，即每个查询请求从提交到执行必须经过根据策略选择模型变体这一步骤

  2. 负载变化(这里的扩展可以是增加也可以是减少)

     当负载发生变化时，Model-Autoscaler使用该策略来决定是否需要扩展正在运行的变体，或是启动其它的变体。Model-Autoscaler通过监控和对比INFaaS的到达负载和当前的吞吐量，决定是否需要进行扩展。当负载变化导致需要进行扩展时，Model-Autoscaler在后台采用该策略来选择一个合适的扩展方案

- Metadata Store

  Metadata Store支持其它组件快速地查询Worker和模型变体的静态、动态信息，从而帮助快速进行决策。Metadata Store存储的信息包括模型变体的相关信息(如准确率、推理延时、负载情况)以及工作设备的相关信息(如资源利用率和负载情况)。Metadata Store通过使用数据结构来降低信息访问的时延，且与Controller运行在同一个机器上从而降低通信时延

- Model Repository

  模型仓库是一种大容量的持久化存储介质，用于存储序列化的模型变体，并在需要时提交给对应的Worker Executor，部署并运行模型变体



#### Selecting and Scaling Model-Variants

- 选择策略简介

  INFaaS引入了内部接口getVariant函数，来调用模型变体选择策略。选择策略必须考虑模型变体和可用资源的静态、动态状态，否则(只用静态状态)会严重地影响策略的正确性。只考虑静态状态时，异常情况会导致决策不准确，包括：选择的变体未被载入，需要考虑载入时延；变体在负载峰值下提供服务；变体与其它推理服务发生资源竞争；变体所需的资源不足导致无法载入。INFaaS通过以下方式来追踪模型变体的动态状态

  1. 引入模型变体的生命周期的状态机

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220905213449501.png" alt="image-20220905213449501" style="zoom:50%;" />

     INFaaS通过引入表示模型变体生命周期的状态机来表示每个模型变体的动态状态，如上图。所有的模型变体最初都位于Inactive状态，表示还没有被载入到worker上运行；当模型变体被载入时，它进入到Active状态，此时它的吞吐量低于其峰值吞吐量(由monitoring daemons进行跟踪)；当模型变体接受的负载过多，以峰值吞吐量进行服务时，它将进入Overloaded状态；如果模型变体吞吐量低于峰值吞吐量，但其服务延迟却高于测试延迟时，它将进入Interfered状态，表示模型变体正在竞争资源

  2. 追踪模型变体的状态并维护状态机

     每个模型变体实例的状态机都由Worker中的Monitoring Daemon进行维护，并在Metadata Store中通过良好的组织支持快速访问。这使得Dispatcher能够快速地获取模型变体实例的动态信息，从而依据选择策略做出决策

- Case I: On arrival of a query(请求到达时的模型变体选择策略)

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220905214803970.png" alt="image-20220905214803970" style="zoom:50%;" />

  1. 分配策略解读

     当一个请求到达时，INFaaS的分派器将会调用上述GetVariant方法来选择合适的模型变体来为请求服务。GetVariant方法的输入是查询的需求，并且输出选出的模型变体和运行的worker。首先，由于已经在运行的模型变体不需要引入载入时延，因此该方法首先检查是否有能够满足查询需求的正在运行的模型变体。如果有满足要求的正在运行的模型变体实例，则将当前请求分派到负载最低的实例上运行。如果没有合适的模型变体实例，将考虑没有被加载的模型变体。此时，GetVariant首先访问Metadata Store，获取模型变体的信息，并找到能够满足请求需求的载入和推理时延最低(两者综合考虑，如按比例相加)的模型变体。如果能够找到合适的模型变体(精确度和延迟能达到需求)，则将其分配到硬件利用率(模型变体适用的硬件的利用率)最低的worker上运行，否则选择一个最接近目标需求(精确度和延迟最接近需求)的模型变体来进行服务

  2. 分配策略的可扩展性

     状态机和模型变体选择策略是可扩展的，例如可以通过修改GetVariant函数，更换不同的选择策略(如选择耗电最少的变体而不是负载最低的变体)

- 缓解性能下降的方案

  为了提升硬件资源的利用率，INFaaS会采用硬件共享的方式，将模型变体实例分配到同一个硬件上运行。然而，这种方法往往会造成实例之间的资源竞争，出现互相干涉和违背SLO准则的情况。为此，在请求到达时，分配策略不会将请求分配给处于Interfered和Overloaded状态的实例。对于处于Interfered状态的实例，INFaaS会在后台启动一个缓解进程，避免影响到服务性能。该进程首先检查运行Interfered状态的实例的worker上是否有空闲的硬件能够运行该实例，如果有则将该实例分配给空闲的硬件运行。如果没有合适的硬件，则请求控制器的Dispatcher将该实例转移到相应硬件利用率最低的worker上运行(这个工作应当是停止当前的实例，并在另一个worker上调用仓库中相应的模型变体，启动一个新的实例)。对于处于Overloaded状态的实例，将由Model-Autoscaler对当前情况进行评估，决定是否进行负载变化的扩展

- Case II: On changes in query load(负载变化时的扩展策略)

  随着应用负载的变化，INFaaS需要去重新评估之前的模型变体选择，检查是否能有更具效益的模型变体组合能够进行服务。已有的推理服务系统不考虑模型变体的多样性，因此在负载变化时只通过简单的加减固定的模型变体来进行响应。然而，由于模型变体的合理性取决于当时的负载和需求，且可能出现模型变体所需资源不足的问题，因此只采用单一的模型变体是不合理的

  INFaaS的自动扩展融合了VM级别的水平扩展和模型级别的自动扩展(水平和垂直扩展)，使得INFaaS的自动扩展是由Controller和Worker协作完成。Worker中的Model-Autoscaler通过对比负载和吞吐量，执行模型级别的自动扩展，即将运行的变体更换成其它的模型变体(垂直扩展)或运行新的模型变体(水平扩展)。Controller的VM-Autoscaler根据负载变化，执行VM级别的水平扩展。以下具体说明这些扩展方式采用的策略

  1. Model-Autoscaler at each worker

     为了响应查询负载的变化，INFaaS的Model-Autoscaler需要调整模型变体的组合，从而在满足查询需求的同时，最小化运行模型变体的开销。Model-Autoscaler通过制订一个整数线性方程(ILP)，从而解出最优组合中每个模型变体的扩展方式(增加或减少)。

     对于每一个变体的伸缩带来的开销，可以用如下方程表示

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220906104324948.png" alt="image-20220906104324948" style="zoom:50%;" />

     其中Cost表示i模型的j号变体的伸缩带来的开销($/s)，C表示该变体对应的硬件开销($/s)，δ表示该变体的伸缩情况(正表示载入，负表示卸载)，Tload表示该变体的启动时延。λ是一个可变的参数，在λ值较高时，说明将更多的权重赋予启动时延，目的在于尽可能降低启动时延，以应对难以预测的负载变化；λ值较低时，说明此时负载较为平稳，应尽可能降低维持变体的开销。

     通过如上的计算伸缩开销的方程，我们可以计算出每个变体在伸缩过程中带来的开销。我们的最终目标在于确保满足查询需求的同时，最小化伸缩所有变体带来的开销(卸载变体比维持不变的伸缩开销更小)。在解出这个方程式的最优解的同时，我们需要增加如下约束

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220906105914294.png" alt="image-20220906105914294" style="zoom: 50%;" />

     这些约束被解释为

     (1) Q为ij变体的QPS(吞吐量)，N为ij变体正在运行的实例数量，L为负载到达速率，slack表示可配置的余量(用于应对突然到达的负载)。整个方程表示新的变体组合必须能够为到来的所有请求服务，并且一定程度上能够应对无法预料的负载尖峰。slack的设置需要谨慎，余量设置过大时可能导致过度的资源开销，设置过小时导致出现负载尖峰时服务违背了SLO限制

     (2) Tinf表示ij变体的推理时延，S表示需求SLO(时延限制)。整个方程式表示任何使用的变体都应当能满足时延的SLO，即能在规定时间内响应请求

     (3) Rij表示ij变体的资源消耗量，Rtotal表示某类资源的总量。整个方程式表示最终方案每个资源的使用量不应超过该资源的总数

     (4) 该方程式表示每个变体的运行数量必须是非负的

     不幸的是，这个ILP是一个NP难问题，需要进行穷举遍历所有的模型变体，获取它们的动态状态，并准确地预测它们的吞吐量QPS，从而找到一个合适的伸缩方案。这个穷举遍历的时间是非常长的：Gurobi花费了23秒才能在50个模型架构下找到最优的伸缩方案。由于模型推理服务是实时性的(特别对于一些时延敏感型应用)，INFaaS必须对负载变化有亚秒级的响应时间

  2. A Greedy Heuristic(采用启发式的贪心算法解决问题)

     由于通过解ILP问题来获得伸缩方案的方式并不适用，我们转而采用一个启发式的贪心算法，达到在亚秒级延迟内满足概述约束的目的。该启发式算法通过修剪搜索空间，采用模型变体的子集来降低决策时间。基于启发式的贪心算法，Model-Autoscaler采用以下选择策略来逼近ILP

     (1) 通过计算确认哪个约束已经有可能被违背

     (2) 考虑采用水平伸缩或垂直伸缩来满足约束

     (3) 计算出每种伸缩行为的方案(变体的增减方案)，并选择能最小化硬件开销的方案

     (4) 如果约束不能通过模型级的伸缩满足，则通知controller调用VM级的伸缩

     **事实上，ILP中的约束(2)和(4)都可以通过简单的方法避免，主要考虑的是(1)和(3)约束。约束(1)被违背的情况往往是负载增加导致，此时需要进行扩展。约束(3)被违背的情况往往是负载过度增加无法通过扩展满足(此时应当调用VM级的伸缩)，或是存在硬件不可用的情况(此时需要减少实例)。另外，减少实例的方案还可能用于降低开销。因此，约束的违背最终能通过两种伸缩算法解决，即Scaling up algorithm和Scaling down algorithm**

     具体的伸缩算法为

     - Scaling up algorithm

       为了确定是否需要进行扩展(约束1被违背)，Model-Autoscaler通过计算出当前worker的最大吞吐量，以及当前worker的请求到达数量，并以两者之比作为指标进行决策。worker的最大吞吐量可以通过累加所有运行实例对应模型变体的测试吞吐量计算得到，worker的请求到达数量可以通过累加所有运行实例的负载到达数量得到。Model-Autoscaler通过设定一个阈值slack-threshold，在计算出最大吞吐量和请求到达数量的比值后，将比值与阈值进行比较。如果比值低于阈值，则Model-Autoscaler调用扩展算法，对当前的模型变体进行扩展

       扩展算法通过getVariant函数表示，其输入为到达整个Worker的查询负载，输出为合理的伸缩策略。函数具体的运行流程为(该算法是启发性的，文中并没有说明具体的操作)：

       (1) 扩展算法首先计算水平扩展的方案，即估算需要增加的正在运行的模型变体(复制当前的模型变体)，使增加实例后所有运行的实例足够为所有到达的负载提供服务

       (2) 接下来，扩展算法通过查询Metadata Store，从中筛选出能够满足SLO需求且吞吐量大于正在运行的变体的模型变体，获得一个合适的变体组合，并估算出各个模型变体的实例数量(垂直扩展)

       (3) 最后，对比水平扩展和垂直扩展的方案，计算出两种方案的总体开销(包括运行开销和启动开销，参照ILP的目标函数)，根据开销大小决定复制模型变体还是替换模型变体

       (4) 由于worker可用的资源有限，因此如果选择的方案需要的资源大于worker拥有的资源，则worker会通知controller将变体加载到合适的worker上或进行VM级的扩展

       **这里的水平扩展方案和垂直扩展方案的求解都是求解ILP问题，从而分别获得水平扩展和垂直扩展对应的最低开销的方案。与最开始讲述的从整体上求解ILP问题不同的是，水平扩展只考虑运行的变体，垂直扩展只考虑满足SLO且吞吐量更大的变体，这使得需要考虑的决策空间变小，能够更快地解出结果。另外，水平扩展和垂直扩展的方案计算不需要考虑所有约束，如水平扩展的方案不考虑资源消耗的约束，使得这种局部的ILP求解更加简单，更容易获得合适的伸缩方案**

     - Scaling down algorithm

       Model-Autoscaler采用与scaling up类似的方法来决定何时进行scaling down和如何进行scaling down(分开水平和垂直扩展)。与scaling up不同的是，scaling down策略每隔一定间隔执行一次(At regular intervals)，检查在卸载一些变体或降级变体(batch大小更小或运行在不同硬件等)后，worker还能否按要求为到达的查询负载服务。Model-Autoscaler在进行变体的卸载之前，会等待Tv时间，避免worker中吞吐量缩减过快。Tv通常等同于变体启动的时间

     

     除了传统的模型水平扩展外，Model-Autoscaler引入了模型的垂直扩展，利用模型变体的多样性(升级成吞吐量更大的模型变体来进行服务)来进一步降低开销。因此，INFaaS的伸缩算法得到的总吞吐量与最优算法相当，而开销位于最低开销和水平扩展之间，有相当的进步。另外，与解ILP的方法相比，Model-Autoscaler采用的启发式算法能够在亚秒级的时间完成决策，避免了决策滞后的问题

  3. VM-Autoscaler at controller

     除了模型自动扩展外，INFaaS同样具有VM自动扩展，即伸缩运行模型变体的worker。参照其它已有系统的扩展机制，VM-Autoscaler采用以下的扩展方案

     (1) 如果所有worker的某一硬件资源利用率都超过了一个可配置的阈值，则VM-Autoscaler会增添一个具有相同硬件资源的worker。考虑到VM的启动时间约为20-30s，作者将这个阈值设置为80%，从而避免了不必要的扩展或是过快的扩展

     (2) 如果对于所有worker的某一硬件平台(如GPU)，都存在Interfered状态的变体运行，则VM-Autoscaler会增添一个包含该硬件平台的worker

     (3) 如果超过80%的worker都存在overloaded的模型变体，则VM-Autoscaler增添一个运行的worker

  
