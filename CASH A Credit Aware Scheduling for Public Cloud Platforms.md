## CASH A Credit Aware Scheduling for Public Cloud Platforms

#### Abstract

公有云提供了许多类型的服务，从而支持它的用户能以灵活、简单和有经济效益地运行大规模的大数据任务。用户通常使用大数据处理框架来处理数据，包括MapReduce、Tez、Spark等。用户可以配置这些框架，通过框架本身运行独立任务，或是应用集群管理器中间件(如YARN、Mesos)，来负责在公有云上的资源调度。集群管理器需要在感知工作负载的需求的同时，检测集群内资源的状态，进行资源配置。云提供商对硬件资源应用了令牌桶机制(token bucket mechanism)来作为硬件提供的服务质量的指标。本文旨在说明如何通过修改集群管理器中间件(如YARN、Hadoop等)，使它们能够感知到集群硬件资源的期望服务质量。通过利用粗粒度的任务需求信息和细粒度的硬件资源期望服务质量信息，集群管理器能够做到良好的任务分配。实验表明基于CPU积分的实例(如Amazon T3实例)是运行大数据负载的可行的具有成本效益的选择，且Hive仓库中流水线化SQL查询能够有31%的加速，并节约22%的开销

**token bucket mechanism是什么机制**

令牌桶机制（Token Bucket Mechanism）是一种用于流量控制的算法。它通过限制数据包的传输速率来防止网络拥塞，常用于网络带宽管理和限速控制。这种机制允许在短时间内超速发送数据（如果令牌积累），同时保持长期的平均发送速率。

- **令牌桶**：在系统中设立一个虚拟的“桶”，其中存放一定数量的“令牌”。
- **令牌生成**：令牌按固定速率生成，并放入桶中，桶的容量有限。
- **数据包发送**：每个要发送的数据包都需要消耗一定数量的令牌。只有当桶内有足够的令牌时，数据包才能发送，否则数据包会被延迟或丢弃。



#### Introduction

大数据工作负载通常运行在计算集群上，其处理的数据集要么分布在大量的存储卷上，或是集中在某个存储服务中。大数据处理任务往往需要大量的计算资源来处理数据，而在云环境下可以很容易地获得大规模的计算资源和存储资源。因此，如何在云环境下以经济高效的方式运行大数据工作负载成了重要的研究工作

相比于私有数据中心，公有云在运行大数据处理任务上更具备灵活性和经济效益。公有云允许用户根据自己的工作负载需要，以固定时间的方式或弹性时间的方式，租赁相应硬件和软件服务，减少了为额外资源偿付的开销。相比之下，传统的私有数据中心要求用户为其完整的生命周期进行付费，使得私有数据中心的资源在任何时候未被使用都引起了额外的开销。另外，为了避免违背SLO，数据中心需要持续配置峰值负载需要的资源，造成了大量的浪费。而公有云下，用户可以根据负载情况弹性地设置资源数量，灵活应对不同的负载要求，从而减少开销(这一段提到的资源应主要为计算资源，如CPU、GPU等)

除计算资源外，公有云还提供低代价、高可靠性和高可访问性的存储方案。用户可以在公有云存储服务上存储任意大小的数据，并按确切的存储数量进行计费，不会因为预先准备存储导致浪费和支付额外开销的情况。相比为硬件资源支付的方案，为存储的确切数据量支付的方案更具有灵活性和经济效益。然而，除了存储资源外，公有云提供商往往还需要用户为数据传输进行支付，且支付的费用依赖于用户与数据中心的距离。因此，用户往往倾向于将数据存储在与自己相近的数据中心，从而减少因数据传输导致的费用

MapReduce、Spark、Flink等均为当前热门的大数据处理框架。这些框架将大数据处理任务划分为多个子任务，并将有相同特性的子任务合并为复合任务，以复合任务为单位进行调度运行。框架在为任务生成执行计划(如规定任务的运行顺序)后，可以利用内置的任务调度器，对任务进行调度运行，也可以将任务提交给额外的调度器进行调度运行

集群管理中间件，如Kubernetes、YARN，通常作为资源管理器，为大数据处理框架的任务分配资源，支持任务的运行。中间件的调度策略往往假设硬件资源的服务率是固定的，但在公有云环境下，由于硬件资源可能会被共享，会出现硬件资源服务率可变的问题(例如，资源分配时，分配一个CPU核往往假设该CPU只为该任务工作，服务率为100%，但是在公有云环境下，其服务率可能下降为80%)。如果不考虑硬件资源服务率可变的问题，往往会引起次优(sub-optimal)的资源分配。特别对于IO、网络这类资源，它们往往不会被考虑在资源分配中，它们可变的服务率使任务如果被分配到这些资源紧缺的结点，任务的运行将会被阻塞

公有云提供商通过SLA揭示了服务率的可变性。提供商会对硬件产品的可用率进行评估，而用户可以利用提供商的评估结果来确定这些实例的资源的期望服务率。在AWS中，通过token-buckets机制，用户可以主动地查阅特定IT资源的token的状态，从而确定其期望的服务率。例如，用户可以利用公共的API，查阅AWS中AWS T3 burstable实例的CPU的token状态。

本文提出了credit aware scheduler-CASH，通过考虑VM相关的token-bucket状态信息以及需要调度的任务的资源需求，对应用任务进行调度。作者在YARN上部署了CASH，向云提供者查询对应结点的token-bucket状态以及从Tez、Hadoop等应用框架获得任务的信息

CASH的主要优势在于

1. 通过将硬件资源相关的token-bucket状态信息考虑入任务调度策略，节约了运行开销并加速了任务运行
2. 证明了AWS的T3 burstable实例是运行基于batch的大数据负载的可行且经济的方案
3. 平均减少了streaming SQL queries任务31%的运行时间，并降低了所有SQL负载总运行时间的22%(因此降低了22%的开销)



#### Background

本节主要介绍AWS的几种服务产品

- AWS T3 burstable instances

  AWS T3是基于CPU积分的实例，它能够确保CPU能以baseline的CPU服务率运行，通常指的是单个通用vCPU的比例(如确保30%的vCPU能够为任务服务)。当实例以baseline以下的负载运行时，它能够积累CPU积分credit，直至相关的token bucket已满。每个积分能够使实例在一分钟内占用100%的CPU或在2分钟内占用50%的CPU。对于每个实例，它获得的积分以毫秒millisecond为单位并可以通过Amazon提供的API进行跟踪查看。T3同样支持非限制的选项，避免用户在用完积分后被强制限制在baseline的服务率下。积分余额按24小时或实例生命周期(推测是将积分余额均摊到实例的计价周期，平均超出的量增加到计价周期单价中)的平均值来算，用户需要为使用的额外积分进行付费

- AWS Elastic Block Storage (EBS)

  AWS EBS服务可以被关联到任意的EC2实例中，其提供可靠的存储卷且生命周期依赖于关联的AWS实例。EBS主要分为两类：SSD backed和HDD backed。存储卷的性能随着其大小提升，且SSD backed类的存储卷能够提供远超HDD backed类的存储卷的IOPS(IOPS为每秒的读写次数，SSD类价格约为HDD的2倍)。然而，EBS卷的性能和服务率与其他公有云产品一样，具有变化性。因此，EBS采用类似的token bucket机制，来为用户提供服务。对于每GB的EBS卷，EBS服务能够提供3IOPS的基准传输速率。当用户以低于该基准的速率使用EBS卷时，存储积分将被添加到EBS卷的token bucket中。用户可以在需要时利用积分，最高达到3000IOPS的传输性能

-  Amazon Elastic MapReduce (EMR)

  Amazon EMR为用户提供了大数据处理服务(SaaS产品)，允许用户运行Hadoop、Spark等应用框架和Hive等数据库来处理大数据。EMR能使用户拥有一个正在运行的数据处理集群来运行工作负载，并内置YARN调度器作为集群管理器。用户可以选择创建集群的硬件类型，定义实例类型和对应的存储服务，来进行集群创建。EMR的限制在于，它不允许用户使用T3 burstable实例来创建集群

  **EMR应当是允许用户定义集群的规格、使用的实例类型等，帮助用户创建好相应的集群，并根据用户需要安装用户指定的应用程序(如Hadoop、Hive、Spark等)。用户通过控制平台，启动应用并提交数据集，从而执行数据处理操作。相比直接创建实例构成集群，EMR应当简化了集群的创建和应用程序的配置等，管理等操作偏自动化**

- Amazon S3

  Amazon S3服务时面向对象的存储产品，提供廉价且可靠的存储服务。S3允许用户存储任何大小的数据，并能通过Internet在任何地方访问它。因其低价、高可访问性以及存储的灵活性，用户更倾向于使用面向对象存储S3来进行数据存储，用于运行大规模大数据负载。相比于SSD backed的EBS，S3每GB的存储价格更低(0.1比0.023)，但每次对面向对象存储的调用需要0.0004-0.005$，取决于存储可用区的距离



#### Motivation

本节介绍了目前集群调度和资源分配存在的问题

- CPU利用率问题

  大型计算集群往往面临着低的资源利用率和低的资源效率的问题，即使他们将在线服务组合到同个位置或是将工作负载进行batch批处理。例如，阿里巴巴的数据显示集群中有60%的机器利用率低于40%，且75%的机器利用率低于50%。然而，由于CPU的使用可能会在某个时段爆发式增长(利用率超过60%)，集群必须配置足够的CPU资源来避免在爆发增长时违反SLA。CPU利用率低的另一个原因是用户倾向于使用廉价的外部面向对象存储，而不是具备locality的存储资源，增加了通信开销，使得CPU计算的资源利用率底下

  1. EMR上的CPU利用率检测

     作者在EMR上测试MapReduce的性能效率，得到如下结果。可以看到对于每个结点而言，CPU利用率较低，平均只有30%。而检测CPU的利用情况，发现各个节点的CPU利用率随时间变化都有较大的起伏，这主要的原因在于采用了外部的面向对象存储，需要较大的读写时延。一种可能的增加CPU利用率的方法是减少供应的实例数量，但这会减少并行IO的数量(一次IO调用申请的数据量减少，按调用次数计费时增加了调用的次数，将会造成浪费和增加开销)，降低了IO的性能

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220924113734825.png" alt="image-20220924113734825" style="zoom:50%;" />

  2. 采用基于积分的实例的情况

     采用基于积分的burstable实例能为平均CPU利用率低的大数据负载节约很多的开销。相比于同等配置的普通实例，爆发性实例的单价更低，这使得用爆发性实例配置具有同等峰值资源的集群能够节约大量的开支。平均利用率低的大数据任务在大多数情况下，CPU利用率低于爆发性实例的baseline，从而能够积累积分，在利用率陡增时能够利用积分提供峰值的CPU性能。这说明爆发性实例在降低了开销的情况下，还保证了服务的SLA，是适用的实例类型

- IO利用率问题

  对于IO密集型负载而言，磁盘的传输效率是重要的性能瓶颈。分析TPC-DS的实验表明在大数据系统上运行该负载引起了极高的IO利用率(甚至到达了100%)，且新的研究表明在当代的产业集群中，IO利用率通常较高。对于IO密集型负载，如SQL查询，磁盘的IO速率往往影响了CPU的利用率和任务完成的效率

  对于EBS这类与实例通过网络关联的存储服务，云提供商采用基于积分的策略进行服务供应，根据积分提供相应的存储服务率。对于IO密集型负载，它们往往需要使用存储积分来获得高的存储IO速率，将它们分配到积分较多的实例上会增加其运行效率。然而，当前的集群管理器，如YARN，通常采用随机分配的方式，将任务分配到任一节点上，而不考虑该节点是否有剩余的存储积分，或是否已经因CPU或IO利用率过高导致阻塞。这种调度方式无法充分利用上集群中可用的存储积分，可能会使得IO密集型任务无法有良好的运行效率(随机分配的方式使得节点上的积分具有不平衡性，出现了部分节点存储积分多部分存储积分少的情况，这可能也会影响任务的运行)



#### Proposed Scheme

本节提出一个调度方案，结合了对任务需求的感知和对资源积分的感知。集群管理器通过从应用框架获取任务的需求注释，确定那些应用需要burst credit来运行。此外，管理器周期性从公有云中收集各个节点的burst credit的信息，并用这些信息将任务调度到合适的节点上运行

- Task Annotation(任务的需求感知)

  CASH支持用户将他们的任务注释为对某种资源有密集需求，如CPU密集型、disk IO密集型等。用户可以自由地将它们的任务与任意一种资源密集型特征进行关联。但是，CASH的设计之初希望能减轻用户在注释负载方面的负担，通过自动化注释的方法将任务进行标注。注释工作通过框架协助，主要依据是任务关联的DAG节点的特性。如果DAG节点关联的任务是涉及到大规模数据处理的(如"lambda"、"tokenize"，文中称为map-like任务)，那么这些任务因其高资源需求可以应用burst credit来提升应用性能。例如，数据库查询任务通常需要读入大量的数据进行搜索，因此将其分配到具有较多disk credit的机器上能有更好的运行效果

  对于一些reduce-like任务(进行结果收集工作)，如reduce、shuffle，通常不需要过多的计算资源和存储资源，因此可以被分配到CPU/disk阻塞的机器上运行。然而，这些任务需要从各个节点收集相应的计算结果等，通常对network具有较高的需求，因此被框架附上了“network”的注释。

- Credit based scheduling(基于积分的调度)

  作者通过修改YARN的调度器，使其能够基于burst credit(CPU积分或disk积分)的余额进行决策考虑。下图是修改后的调度器CASH的组件与操作。调度分为两种情况考虑：基于CPU积分调度T3 burstable实例或基于disk I/O积分调度常规实例(AWS M5)，每种情况都会考虑到job中不同类型的task。集群中，每个节点有一定数量的槽slot(每个vCPU对应一个槽)，每个slot可以对应运行一个任务。所有应用框架中的任务都被集群管理器组织为一条单一的任务队列进行调度，最后被分派到可用的slot中运行

  **这里提到的两种情况，是指调度器会将集群中的T3实例和M5实例分别按照积分余额进行排序，将CPU密集型任务提交给T3实例，disk IO密集型任务提交给M5实例；还是说集群只由T3或M5实例构成，因此调度器调度时只考虑任务的CPU特性或disk IO特性**

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220924184948104.png" alt="image-20220924184948104" style="zoom:50%;" />

  具体的调度算法如下。每个较长的时间周期(每分钟)，调度器会将节点依据其积分状况(CPU积分或是IO积分)进行递减排序(根据上图，每过五分钟获取真实情况，每过一分钟推断余额)。每个较短的时间周期(毫秒级)，集群管理器按burst credit数量递减顺序查看节点，并尽可能地将密集型任务(CPU密集型或IO密集型)分配给前面的节点。只有某个前列的节点的所有slot被填满后，才会转而考虑下一个节点。当所有密集型任务被分配或slot被填满后，进入下一个阶段。第二阶段中，集群管理器按积分数量递增查看节点，将被赋予了network标签的非密集型任务分配给各个节点。由于这类任务往往是network sensitive，需要考虑负载均衡的问题，因此每一轮节点的遍历，每个节点最多有一个slot被分配来运行任务，以将所有network型任务均匀分配到不同节点上。在最后阶段，如果还有剩余的任务(这部分任务可能是没有被注释为特定类型的，或是因为slot不足剩余的)，则将任务随机分配到可用的slot上

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220924184937298.png" alt="image-20220924184937298" style="zoom:50%;" />

  

#### Implementation

作者在YARN的调度器的基础上实现了credit based任务调度器的原型，并在YARN上部署了Tez和Hadoop应用框架进行实验。Apache Tez框架进行了修改，以使其能够注释任务并且将注释传递给YARN。YARN可以直接利用Hadoop的节点标记功能获得对节点的注释

集群节点的积分余额更新由YARN调度器的一个异步线程完成，更新方式如下：每过五分钟，异步线程从公有云Cloudwatch处获取各个节点确切的积分余额，并更新各个节点对应的数据结构，用于未来进行决策(5min的原因在于Cloudwatch每过五分钟对积分余额进行一次更新)。然而，为了避免以过时的积分余额和数据来进行决策，每过一分钟，异步线程从公有云处获取各个节点的CPU利用率和磁盘的读写情况，并依此作为依据进行积分余额的预测。由于Amazon发布了基于实例/disk大小和CPU/disk利用率的实例积分余额的计算公式，我们可以较为准确地预测出各个节点的积分余额。具体的更新算法如下(可以看到，调度器的调度只依据CPU积分和disk积分的其中一个，用if-else语句进行分割)

<img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220925111100073.png" alt="image-20220925111100073" style="zoom:50%;" />

Tez采用VertexManager的概念协助用户对任务的调度和运行进行动态调整。Tez将内置的vertex-manager基于任务的不同属性关联给不同的节点，使得所有具有特定属性的节点(如CPU密集型)，它们会由同一vertex-manager进行管理。作者修改了两种vertex managers，使它们能够对任务进行注释，以适应调度策略(根据burst credit和任务特性进行调度)：RootInputVertexManager对应的是接收源输入数据并进行复杂数据处理的任务，它们需要利用CPU/IO credit加速运行，被标记为"Disk"或"CPU"；ShuffleVertexManager对应的是接收各个节点的输出数据进行汇总的任务，因此将这类任务进行负载均衡，均摊网络带宽能有良好的效果，被标记为"network"。此外，作者还修改了vertex manager的基类，使用户基于该类自定义的vertex manager能够为相应的节点按照需求进行特性注释

<img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220925122351177.png" alt="image-20220925122351177" style="zoom:50%;" />

对于Hadoop而言，它会为相应的节点进行标记，因此不需要额外的修改。Hadoop的任务可以分为两个部分“Map”和"Reduce"。对于Map的节点，它们需要执行大规模的数据处理，我们将其与burst注释相关联，在调度时尽可能让它们能利用上credit进行加速；对于Reduce节点，它们主要进行收集操作，将其与network注释相关联，通过负载均衡使它们充分利用上网络带宽



#### Evaluation(CPU burst)

- 测试工具与负载

  本文测试采用Intel的HiBench大数据基准测试工具，进行多种工作负载的运行，评估CASH的效果。评估采用的工作负载为PageRank、K-means clustering和Hive SQL aggregation。HiBench会根据需求生成相应的测试数据，并写入S3中作为所有实验测试的输入数据。对于每个HiBench的工作负载，它们都由多个job组成，按顺序提交给框架进行执行(先后顺序由数据依赖关系决定)。每个job被提交给YARN后，会根据其DAG结构划分成多个task，这些task采用本地EBS临时存储数据

  **HiBench的意义在于内置多种大数据应用，简化了用户撰写、编译、打包等步骤，且能自主生成对应的输入数据。实验只需要利用实例搭建好对应的集群，并在上面运行HiBench，便可以测试集群的运行效果**

- 实验设置

  实验的基准系统为配置为Hadoop集群的Amazon EMR集群，在上面运行HiBench并从S3中获取输入数据，测试各个应用在集群上的运行效果；对于测试的CASH的集群，作者在具有超过10个T3实例的集群上配置了Hadoop架构，并采用与EMR集群相同的配置。CASH基于YARN的调度器运行在Hadoop集群上，基于burst credit进行任务的调度

  实验一(naive)：首先在集群上运行平均CPU需求大于基准CPU服务率40%的SQL aggregation任务，令其在受限的CPU下运行(没有可用的burst credit)，之后再运行PageRank和K-means

  实验二(reordered)：先运行PageRank和K-means，在积累一定的burst credit后再运行SQL aggregation任务，使开始时SQL任务不会因为缺少burst credit而受限制。

  实验三(unlimited)：不限制burst credit的使用，使得在没有credit时，负载也能按照需要的CPU服务率进行运行。CPU credit以24小时为周期进行结算，转换为平均的CPU利用率。如果24小时内平均CPU利用率高于基准线，那么用户需要为超出部分进行支付。根据统计，如果CPU24小时的平均利用率为52.5%，T3实例的价格与常用实例价格相同

  **以上部分基于的是初始的YARN调度器，调度不会感知credit的情况**

  实验四(CASH)：本实验修改了YARN的调度器，使其能够感知burst credit的余额和网络敏感型任务，检测调度效果

- 实验结果

  实验的结果不考虑从S3存取数据的时延，对比EMR和另一个集群下负载的运行效果。相比于naive、reordered和unlimited，CASH具有最低的累积运行时间，虽然EMR运行时间减少了13%，但是CASH实例的价格降低了30.7%，使得在CASH运行能够有更低的开销(由于各个步骤重叠的原因，EMR运行时间可能没有降低13%这么多，因此CASH在价格上更有优势)。CASH相比reordered，CPU的平均利用率更低，而EMR则表现出了更好的负载均衡。CASH相比unlimited，由于YARN随机分派的策略，导致unlimited策略下即使其他机器有剩余的credit，任务还是有可能被分配到没有credit的机器上运行，使得需要支付额外的费用，价格上的表现不如CASH，且实例之间的credit偏差较大。

  



#### Evaluation(disk burst)

- 测试工具与负载

  实验利用hive-testbench，基于标准的TPC-DS查询生成Hive的基准测试数据，存储在HDFS(作为hive的数据仓库)中。通过在对应的Hive上顺序执行TPC-DS中三种需要读取大量数据的查询：query66、query49和query37，构建出需要大量disk credit的情况，评估CASH的运行效果

- 实验设置

  实验设置标准YARN和将调度器改动为CASH的YARN，通过运行相同的查询，对比每种查询的运行时间和端对端时间。实验将三种查询同时进行，为了避免缓存影响，关闭hive的缓存，每轮实验之间重启相应实例，并采用不同的输入数据来避免disk的缓存。实验开始时disk credit被设置为0

  不同的实验的设置仅仅在集群大小，hive数据库大小和每个实例的EBS大小上有所不同，总体设置类似

- 实验结果

  对比每个query的完成时间，CASH在任何集群配置下，相比标准的YARN都有更好的性能表现。在较小的集群规模和hive磁盘大小下，性能提升较少，因为查询所需要的磁盘IOPS不高，不需要太多的disk credit。随着集群规模和hive磁盘大小的提升，CASH在性能提升上更为明显，且不同实例之间的credit偏差更低，表现出更好的负载均衡(CASH能够将查询任务分配给具有更多credit的实例，拥有更好的性能)。根据结果推测，运行的负载查询量越大，CASH能够提升的性能和降低的开销越多，优势更为明显

