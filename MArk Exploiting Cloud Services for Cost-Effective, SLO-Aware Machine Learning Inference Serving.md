## MArk: Exploiting Cloud Services for Cost-Effective,  SLO-Aware Machine Learning Inference Serving

#### Abstract

- 背景：随着机器学习技术的不断发展，在云上部署训练好的机器学习模型并对外界提供大规模、低延时的推理服务(ML-as-a-Service)已成为一个重要的需求。ML-as-a-Service面临的挑战在于，如何在变化的工作负载下用较低的成本实现低延时(SLO需求)的推理服务

- MArk解决方法：动态地将工作负载组织成批，按机会使用硬件加速器(如GPU)运行成批的工作负载，提升性价比；由于采用反馈控制机制调整工作负载会导致滞后的控制和昂贵的开销，MArk采用预测负载、自动伸缩的机制来低成本地隐藏工作负载调整的延迟；对于难以预测的偶然的爆发式负载，MArk使用灵活但昂贵的serverless实例来运行这些负载

- 效果：在一些最先进的机器学习模型的服务性能上，相比产业界主流的机器学习服务平台SageMaker，降低了7.8×的开销而达到了更优秀的性能



#### Introduction

- 系统目标(关注目前的重要需求)

  目前，如何高效地在云平台上训练机器学习模型已经得到了很好地解决。机器学习推理阶段的工作在于，利用一个使用已有数据集训练好的模型，根据终端用户的输入，得到相应的推理结果(例如人脸识别)。推理工作必须是实时响应的，其响应时间需要满足SLOs准则(例如98%的请求能在200ms内得到响应)，否则会导致服务质量的下降乃至经济损失。因此，对于ML模型服务系统，目标在于达到SLOs准则的同时，最小化提供服务实例的开销

- 面临的挑战(研究提出的原因)

  1. 云提供商提供的服务和计价模型层出不穷。Amazon、Google等云提供商提供了众多服务选项，包括虚拟机、容器、serverless函数等。对于每种选项，其实例的创建有极大的配置空间(CPU、内存等)和众多的计价模型，使得价格和性能之间的权衡变得极其复杂
  2. 缺乏指导和经验。云提供商没有提供通用的配置和权衡方面的指导，且先前没有针对一般工作负载的研究

- 调查和发现(实现方式的依据)

  1. 对比分别使用IaaS、CaaS和FaaS的性能和开销，得到的结果是：IaaS的性价比最高，实例创建时延高且对负载变化的敏感度较低；CaaS性价比低于IaaS，但创建时延和敏感度略优于IaaS；FaaS的扩展效率最高，但是非常昂贵(采用结合的方式，平衡性能和开销)
  2. 通过组织成批并运行在硬件加速器上，推理服务可以获益良多，但是获益程度与批处理的大小等紧密相关。因此，系统应能够明智地决定何时将CPU实例扩展到GPU和如何在GPU上实现批处理(组织成批从而利用硬件加速器提升性价比)
  3. ML推理服务通常是无状态的，使得使用serverless实例进行额外扩展变为了可能；很多ML推理任务的执行时间(确定的输入以及与输入无关的运行流程)是可预测的，使得资源预测和延迟控制变为了可能

- 具体实现

  MArk允许推理服务开发者通过通用API设置目标SLOs，以定义响应的时间等；为达到高性价比，MArk使用IaaS作为基础供应方式，并使用FaaS的方法来快速响应资源空缺；MArk采用预测扩展机制，掩盖IaaS对负载变化不敏感导致的延迟，并用serverless实例来弥补超出预期的负载；基于对负载的预测，MArk视情况将负载组织成批交由GPU进行运行，以达到高性价比；MArk还使用中断容忍机制利用可中断的、低成本的实例，进一步降低开销(这个方法不是很理解)



#### Background and Related Work

- 已有的推理服务系统

  1. 将训练好的模型部署在容器中，并通过REST API进行请求处理，如Clipper、Rafiki等将模型部署在Docker容器中实现隔离。这部分系统通过模型无关或模型相关优化技术，得到低延时的推理服务。这部分系统主要关注简化服务器机器中的模型部署，没有解决可扩展性和开销最小化的问题
  2. Microsoft的Swayam系统专注于基础设施的扩展和资源效率。但是该系统是用于在 Microsoft 的私有 MLaaS 集群中部署模型的专有系统，他们的计价模型与云交付选项(IaaS、CaaS、FaaS等)无关(私有云上运行的系统)
  3. Amazon的SageMaker提供了可扩展的模型服务，但是它只提供IaaS交付方式且需要手动规定供应实例(manual specification of the provisioning instances)。同时，它只提供力所能及的交付，而忽略了响应时间的SLOs准则

- 自动化伸缩机制

  1. 反馈控制伸缩

     监测应用的运行状况，根据监测得到的监控指标进行资源调整。该机制被许多产业界服务平台所采纳，如SageMaker和Kubernetes。伸缩的依据是一些自定义的规则，如实例的利用率、实例每分钟处理请求的数量

     反馈控制伸缩不对未来使用状况进行预测，易于部署。但是当工作负载变化时，该机制会有较大的实例伸缩延时。对于负载尖峰的情况，该机制需要进行过量配置。然而，由于模型推测通常是计算密集型且需要昂贵的CPU/GPU实例，依靠过量配置来解决尖峰问题是经济不适用的

  2. 预测伸缩

     预测伸缩机制预测未来的负载情况，并以此为依据自动伸缩实例，从而避免过量配置over-provisioning的情况。基于一系列时序预测算法的预测伸缩机制已经广泛用于服务常规的工作负载，且常与反馈伸缩机制进行相互补充，例如预测伸缩用于进行未来几小时或几天的整体资源规划，而反馈伸缩机制运行周期为几分钟，用于响应突发情况和意外偏差

     然而，由于已有的预测伸缩机制依据的工作负载与推理系统不同，已有的预测伸缩器不适用于推理系统。这些预测伸缩器要么只考虑了同构实例的情况，要么不支持硬件加速器，要么不支持开销低的可中断实例，要么没有考虑响应时间SLOs

- 云供应方式

  1. Infrastructure-as-a-Service (IaaS)：在IaaS下，云用户通过运行配置好(CPU、内存、存储等多项配置)的虚拟机，进行模型推理软件的配置和部署，并响应模型推理请求。IaaS提供了灵活的实例订购方式，供消费者在价格和服务质量之间权衡。供应方式有按需计费(on-demand，价格最高，灵活性最强，可以根据计算强度或时间进行计费)、可中断实例(实例可能无限期中断，但价格打折)、长期预定(预定某一个时间段，如包月、包年，平均价格低于按需使用)
  2. Container-as-a-Service (CaaS)：在CaaS下，用户将服务封装和运行在容器中，并在云上以指定配置运行容器。CaaS简化了软件配置和基础设施的维护，按需计费(根据容器的使用时间和使用配置定价)
  3. Function-as-a-Service (FaaS)：在FaaS下，用户以serverless函数的方式运行应用，不需要配置或管理服务器。FaaS以请求数量和计算时间为依据进行计费。FaaS适用于无状态应用



#### Characterizing Model Serving in the Cloud(AWS平台多种云供应方式的对比)

- 云供应方式的选择(实例在哪里运行，通过对比三种供应方式，了解不同供应方式的优点，从而设计出能够取长补短的调用方式)：

  分别将选好的ML模型交由三种云供应方式进行运行，并测试响应1000000条请求下的平均开销以及平均响应时间，得到的结果为**(为什么FaaS的处理时延会更高)**

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220822202038182.png" alt="image-20220822202038182" style="zoom:50%;" />

  对于IaaS的EC2服务，本文的实验采用了性价比最高的实例c5.large作为参考；对于CaaS的ECS服务，本文实验采用了与c5.large相同的容器配置。对比IaaS的EC2服务和CaaS的ECS服务，两者具有相近的响应时间，但EC2的价格更低，性价比更高

  对于FaaS的Lambda服务，计费是以处理的请求数来算，且每个请求的单价由为请求分配的资源和处理时间决定。在最优性能下，Lambda时延仍旧大于EC2，且价格更高

  对比三者的可扩展性：EC2具有较长的配置开销，需要较多的时间将大型的ML模型部署在虚拟机上。因此，除非提前进行超量配置，否则EC2无法响应请求尖峰，导致响应时间无法达到要求。另外，在EC2反复配置和取消实例会造成额外开销，因为在模型部署过程中也需要计费。对于ECS，其可扩展性与EC2类似，也需要数十秒的配置开销。相比之下，Lambda可以在几秒内扩展出数千条推理实例，且实例创建后能够无额外开销地持续处理请求。然而，Lambda具有冷启动的问题，即serverless实例在请求进入时，需要检查是否有正在运行的微容器，没有则需要启动容器导致出现额外开销。冷启动的问题可以通过预热的方式降低额外开销

  总体而言，我们可以通过结合IaaS的高性价比以及FaaS的高扩展性，运行ML模型推理服务。对比传统的需要依赖超量配置处理请求尖峰的推理系统，MArk通过以IaaS作为基础，并在IaaS扩充新实例时采用FaaS短暂处理，并利用FaaS处理请求尖峰，达到高性价比和高可扩展的需求

  本小节决定了大的处理方向：基础采用IaaS，并合理利用FaaS。然而，由于IaaS中还具有多种配置方法和计价规则，在IaaS选择合适的服务选项也是一重要命题。以下小节在于确定IaaS的实例配置选择

- IaaS的实例供应：IaaS的供应商通常将实例划分为族，每个族采用相似的硬件配置但是CPU、内存等具体配置不同。在CPU实例上，EC2将其划分为了4类，包括通用实例m-family，计算优化型实例c-family，突发型实例t-family，内存优化型实例r-family

  1. 是否选择突发型实例t-family：突发型实例在业务不繁忙时提供最低的性能，但是在业务负载低于基准性能(AWS为10%)时能够进行算力的积累，并且在出现请求尖峰时实现爆发，消耗累积的算力处理过量的请求。然而，由于累积的算力消耗完后，突发型实例将以很低的性能运行，因此其不适用于计算密集型服务，不适合ML模型推理服务

  2. 实例族和实例大小的选择：本小节主要是对m-family和c-family进行对比(在t-family的实验中，发现内存已经不是瓶颈，因此不考虑r-family)。由于m-family和c-family的实例的价格与实例大小成比例关系，因此在考虑两者的选择时，还需要考虑两者的实例大小情况。本文对最新的两类实例c5和m5进行对比，实验结果为

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220822214729923.png" alt="image-20220822214729923" style="zoom:50%;" />

     总体看来，c5实例在模型推理服务上具有更低的时延和更低的开销，同时，小的实例的性价比更高，且由于更精细的配置粒度，使用更小的实例来服务动态工作负载可以提高资源利用率。综上，采用c5类的小型实例，能有最好的性价比

- 是否使用GPU实现推理加速：

  目前先进的IaaS实例已经配备了能够加速模型推理服务的硬件加速器，如GPU、TPU等。以最普遍使用的GPU为例，GPU实例远比CPU实例要昂贵，但是它最多能够达到40倍的加速。为了能够充分利用GPU的计算性能，需要将多个推理请求组织成批，并使用GPU实现批处理。组织成批地进行处理能够均摊操作的开销，如RPC调用开销、内存复制等，且能够利用上软件、硬件对批处理操作的优化

  本文对c5.large、c5.xlarge、c5.4xlarge以及p2.xlarge(GPU实例)进行比较，得到的结果为

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220823111854475.png" alt="image-20220823111854475" style="zoom:50%;" />

  从实验中看到，批处理对CPU的小型实例没有改进效果，但降低了大的实例c5.4xlarge的平均延时。反观GPU实例，批处理不仅降低了开销，还大幅降低了请求处理的延时。综上，进行合适的组织成批，并将其分配到GPU上进行批处理，能够大幅提升经济效益。然而，组织成批往往带来了排队延时和批量推理延时，因此应细细斟酌批次的大小等因素



#### Characterization in Google Cloud(在Google Cloud上验证主要发现)

- IaaS仍旧是最优的选择：通过对比IaaS、CaaS和FaaS三者的实例在处理1000000条请求时的平均开销和平均响应时间，IaaS的实例仍旧具有最低的平均开销和平均响应时间

- 小型实例具有更好的效果：通过对比不同大小的实例在性能和开销上的表现，得到的结论是即使利用大的实例在性能上具有更好的表现，但是小型实例的性价比更高，且小型实例以其采用精细配置提升资源利用率的特点具有得天独厚的优势，因此小型实例具有更好的效果

- CPU、GPU和TPU的性能对比：实验使用了两个著名的图像分类模型Inception-v3和ResNet50，检验CPU、GPU和TPU在不同batch size下的性能和开销的表现，得到的结果为

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220823120609702.png" alt="image-20220823120609702" style="zoom:50%;" />

  对比CPU和GPU，可以得到与AWS平台类似的结果，即GPU在batch size增加的情况下开销更小，性能更优。查看TPU的表现，发现其高额的开销并不具备超高的性能，反倒是在实验中，其性能始终不如GPU实例。TPU是一种大规模并行加速器，针对训练吞吐量而非推理延迟进行了优化，因此它需要超大的batch size来充分利用其算力。然而对于模型推理这种实时性服务，等待大量的请求进而组织成批是不现实的，这大大增加了排队时延。TPU并不适用于模型推理系统



#### Characterization Summary(对发现的总结)

1. IaaS的实例最适用于模型推理服务。通过将其与FaaS的实例进行结合，可以使得IaaS的实例在面对突发性请求尖峰时具有良好的可扩展性
2. 突发型实例可以用于短暂处理爆发性请求
3. 在按需使用的CPU市场中，虽然IaaS的大型实例有更低的延时，但使用小型实例能够更好的性价比
4. 只有合理地确定批次大小并将请求组织成批，才能充分利用GPU的优势(低延迟和低开销)



#### MArk(具体实现)

<img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220823162656382.png" alt="image-20220823162656382" style="zoom:50%;" />

- overview

  MArk系统采用EC2作为提供ML推理服务的主要方案，并利用Lambda在EC2进行扩张和缩减时快速地补充缺失的性能。用户请求首先被存放在请求队列中，由Batch Manager将它们组织成批。MArk会定期衡量工作负载指标，如请求到达速率，并交由Proactive Controller进行预测和预先规划实例。Proactive Controller根据预测结果启动或删除EC2实例，并执行所有实例的健康检查。每一个EC2实例都具有一个Bouncer，用于监控服务指标和检查请求的处理情况。如果一个请求无法在RTmax时间内得到处理，则会立即将其交给Lambda实例进行处理。此外，MArk还使用了SLO Monitor监控SLO的实现情况

  MArk系统支持的两个SLO指标为：一条指令只有在RTmax的时间内得到处理，其被视为完成；只有百分之SLmin的指令能够完成，才视服务为令人满意的

- Workload Prediction(负载预测)

  为了权衡不同实例和资源配置之间的长期成本，需要使用多步负载预测估计近期的最大请求率。目前已有许多成熟的资源预测算法，但预测算法的效果和精确度依赖于底层工作负载(underlying workload)，不可能适用于任何情况。因此，MArk系统给用户提供了接口，供用户自行选择最合适的预测算法。MArk期望解决的重要问题是在预测的基础上如何去平滑地处理预期之外的负载尖峰。

  在测试中，MArk采用了被证明有最好性能的标准LSTM算法，进行负载预测。Pu代表了每步预测之间的时间间隔，Pw代表了每次预测的步幅，从而每过Pu的时间，MArk的预测器都会预测下面Pu\*Pw时间的请求到达速率的峰值。在每个预测间隔Pu中，预测器会在Ps时间内进行连续采样，计算出Ps时间内的请求到达速率，并在下一步预测时得到上个Pu间隔内的请求到达速率的峰值，进而预测下Pw个时间间隔的速率峰值。在测试中，Pu、Pw、Ps被分别设为1min、60、5s。Pu为1min是因为EC2实例计费至少为1min，Pw为60是因为对未来1小时的预测足以显示未来的趋势，Ps为5s是因为通常短期时间内的请求到达率是稳定的

  MArk的预测方式适用于所有的ML 服务负载预测，因此用户可以微调此预测算法或用他们自己的实现替换它以获得更好的预测结果

- Instance Provisioning and Batching(依据预测配置实例和组织成批)

  鉴于之前的研究表明，即使不考虑请求批处理和实例定价，也无法确切地解决依据预测结果确定配置实例类型和数目的问题(没有封闭结果)。因此，MArk中的算法采用启发式的思想，分别解决batching和provisioning instance的问题

  1. batching

     对于batching操作，MArk引入了两个超参数来控制批处理操作。Wbatch表示batching操作的最长等待时间，Nbatch表示最大的batch大小。根据这两个参数，当Batch Manager等待了Wbatch时间，或者它已经从队列中获得Nbatch条请求时，它会将这些请求组织成批，交给负载均衡器处理。

     两个参数的设置必须满足两个要求：等待时间与批处理时间之和不能超过SLO指定的指令完成时间RTmax，以及等待时间与批处理时间之和不能超过序列化处理请求的时间。用b表示batch大小、Tb表示处理一个batch的时间，μ表示目标实例单位时间内最多能处理的请求数(最优处理速率)，则这两个限制可以表示为

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220823175815733.png" alt="image-20220823175815733" style="zoom:50%;" />

     通过逐步增加batch大小b，直到上述两个限制中有一个限制被打破，我们就获得了最大的可以进行处理的batch大小Nbatch。它与预设置好的Wbatch参数共同指导Batch Manager的工作

  2. provisioning instance：文中设计了一个启发式的贪心算法，用于决定在某个时间点需要启动的新实例和销毁的实例。算法中应用了一个参数τ，表示某个实例的启动到准备完毕需要τ时间。整体算法的目标在于，在t0时刻对t0+τ时刻的需求进行分析，利用贪心算法找到开销最低的实例组，使得这种规划得到的实例组在t0+τ时刻足以处理所有请求。需要启动的实例在t0时刻应当马上启动，但需要销毁的实例可以在等待一个冷却周期后再销毁。具体的算法实现还是要细细阅读论文内容，笔记中不再赘述(但是感觉给的算法有点问题，初始化条件应当是S<-I∪R，此时R表示的是已在运行的实例，这些实例的启动开销为0。FILL部分似乎也有一些问题)

- SLO tracking：如果仅仅只靠提前配置实例的机制来满足需求，将会导致在出现负载尖峰时，多余的请求无法被处理，违背SLO规则。因此，MArk中的Bouncer还会主动监控请求的处理时延，并在SLO规则被违反时马上扩展实例(在扩展过程中，应当是通过Lambda处理这些溢出的请求)。MArk监控每个时间单元的最后几条指令，如果不满足SLO规则，则启动L个T类型的实例将会被启动(默认启动c5.large因为它的性价比最好)

- Spot Instance and Lambda Cold Start(Spot Instance是可被中断的实例类型，但是价格低于按需实例)

  1. Spot Instance以其超低的价格，成为节约开销的重要方案。在上述算法中，并没有将的Spot Instance和on-demand Instance区分开。但是，由于Spot Instance的可中断性(会提前2分钟通告)，必须要考虑的问题是，在Spot Instance被通告中断时，新的Spot Instance实例并不能在2分钟内启动，在这期间如何处理未完成的请求。Burstable instance为这个问题提供了一个解决方案。这类实例便宜，具有快速的启动时间，且能够在短时间内提供尖峰的性能，使得它们非常适合在短期中断的情况下进行请求处理。因此在我们使用Spot instance时，同时保留一些停止的Burstable instance作为冷备用。一旦Spot Instance收到中断通告，则启动Burstable instance进行请求处理，并申请新的Spot instance。只有Spot instance的计算容量恢复正常时，再停止Burstable instance

  2. Lambda实例的冷启动也是需要考虑的问题。在请求发送给Lambda实例执行时，如果此时没有可用的Lambda实例，则需要启动新的实例，这涉及到加载ML模型、框架库等操作，需要较长的时间。然而，冷启动的发生概率很低，一个Lambda实例能够保持45-60min的时间，且研究表明Lambda实例发生冷启动的概率只有0.23%。同时，Lambda实例冷启动的开销也是可忽略的，因此在Provisioning算法中，没有考虑Lambda冷启动的影响。

     然而，MArk中依然给冷启动的影响提供了一个缓和的解决方案。通过预测未来的性能尖峰，并提前通知Lambda实例进行预热(启动)，使得冷启动的影响被降得更低

